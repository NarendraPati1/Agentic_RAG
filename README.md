# ğŸ“š Agentic RAG System for Educational Support

An **LLM-powered Retrieval-Augmented Generation (RAG)** system designed to provide **accurate, context-aware answers** for educational queries.  
Built using **LangChain**, **Google Gemini Pro**, **FAISS**, and **Streamlit**, the project demonstrates how to combine **agents, memory, and tools** for intelligent document-based Q&A.

---

## ğŸš€ Features

- **Agentic Workflow** â€” Uses LangChain Agents to dynamically choose tools based on query type.
- **Context-Aware Answers** â€” Retrieval-Augmented Generation with FAISS for precise responses.
- **Multi-Tool Integration** â€” Supports document retrieval, summarization, and keyword search.
- **Streamlit UI** â€” Clean, interactive frontend for seamless user interaction.
- **Memory-Enabled** â€” Keeps conversation context for follow-up questions.

---

## ğŸ› ï¸ Tech Stack

| Component         | Technology Used |
|-------------------|-----------------|
| **LLM**           | Google Gemini Pro |
| **Framework**     | LangChain |
| **Vector Store**  | FAISS |
| **Frontend**      | Streamlit |
| **Language**      | Python |
| **Libraries**     | `langchain`, `faiss-cpu`, `streamlit`, `pandas`, `dotenv`, `requests` |

---

## ğŸ“‚ Project Structure

ğŸ“¦ agentic-rag
â”œâ”€â”€ ğŸ“„ app.py # Streamlit UI and main workflow
â”œâ”€â”€ ğŸ“„ agent.py # LangChain agent initialization
â”œâ”€â”€ ğŸ“„ retriever.py # FAISS vector store setup and document retrieval logic
â”œâ”€â”€ ğŸ“„ tools.py # Custom tool definitions
â”œâ”€â”€ ğŸ“„ requirements.txt # Dependencies
â”œâ”€â”€ ğŸ“„ .env.example # Environment variable template
â””â”€â”€ ğŸ“‚ data # Educational PDFs and documents

yaml
Copy
Edit

---

## âš™ï¸ Workflow

**1. Document Ingestion**
- Educational PDFs are loaded and split into chunks.
- Chunks are embedded using `GoogleGenerativeAIEmbeddings` and stored in FAISS.

**2. Query Handling**
- User enters a question via the Streamlit UI.
- Query is passed to the **LangChain Agent**.

**3. Tool Selection**
- Agent decides whether to:
  - Retrieve data from FAISS
  - Summarize results
  - Answer directly from LLM

**4. Contextual Answer Generation**
- Retrieved chunks are combined with the original query.
- Gemini Pro generates an accurate, context-aware answer.

**5. Response Display**
- Answer is shown in Streamlit, along with relevant sources.

---
